# Python


## <a name="import">Импорт библиотек</a>

`import pandas as pd`
> импортирует библиотеку для работы с таблицами и данными в виде датафреймов

`import numpy as np`
> импортирует библиотеку для работы с числами

`import seaborn as sns`
> импортирует библиотеку для создания графиков и визуализации данных

`import matplotlib.pyplot as plt`
> импортирует библиотеку для создания графиков и визуализации данных

`import plotly.express as px`
> импортирует библиотеку для создания интерактивных графиков

`import datetime`
> импортирует модуль для работы с датами и временем

`import matplotlib.dates as mdates`
> импортирует модуль для работы с датами и временем в библиотеке matplotlib

`import random`
> импортирует модуль для генерации случайных чисел

`import re`
> импортирует модуль для работы с регулярными выражениями

`import vk_ip`
> импортирует модуль для работы с IP-адресами ВКонтакте

`import os`
> импортирует модуль для работы с операционной системой

`import json as json`
> импортирует модуль для работы с JSON-данными

`import requests`
> импортирует модуль для работы с HTTP-запросами

`import gspread`
> импортирует библиотеку для работы с Google Sheets

`import zipfile`
> импортирует модуль, который предоставляет функции для работы с архивами ZIP в Python

`from io import BytesIO`
> импортирует модуль для работы с бинарными данными в памяти

`import pandahouse as ph`
> импортирует библиотеку для работы с ClickHouse

`from scipy import stats`
> импортирует модуль для статистических вычислений

`from scipy.stats import zscore`
> импортирует функцию для вычисления z-оценки

`import pingouin as pg`
> предоставляет различные функции для статистического анализа данных, включая ANOVA, корреляцию, регрессию и др.

`from scipy.special import comb`
> функция comb из библиотеки scipy.special, которая используется для вычисления комбинаторных коэффициентов

`import statistics`
> библиотека, которая содержит набор функций для выполнения статистических расчетов

`from statsmodels.formula.api import ols`
> интерфейс для моделирования линейной регрессии с помощью формул

`from statsmodels.stats.anova import anova_lm`
> используется для проведения анализа дисперсии (ANOVA) в статистическом анализе данных

`import statsmodels.formula.api as smf`
> работа с формулами и моделями

`import statsmodels.api as sm`
> предоставляет классы и функции для моделирования и статистического анализа данных

`from sklearn.cluster import AgglomerativeClustering`
> используется для кластеризации данных с помощью иерархической кластеризации

`import bootstrapped.bootstrap as bs`<br>`import bootstrapped.stats_functions as bs_stats`
> позволяют оценить распределение статистических показателей, таких как среднее значение, медиана, стандартное отклонение и т.д.

`from statsmodels.stats.contingency_tables import StratifiedTable`
> используется для анализа таблиц сопряженности

`from scipy.stats import chi2_contingency`
> используется для расчета критерия хи-квадрат для таблиц сопряженности

`from df2gspread import df2gspread as d2g`
> функция используется для экспорта данных из pandas DataFrame в Google Sheets

`import psycopg2`
> модуль, который позволяет взаимодействовать с базой данных PostgreSQL из Python

`import warnings`
> модуль, который позволяет управлять выводом предупреждений во время выполнения программы. Например, так:<br>
> warnings.filterwarnings('ignore')

`from oauth2client.service_account import ServiceAccountCredentials`
> используется для аутентификации в Google API с помощью учетных данных службы

`from my_module import my_function`
> содержит функцию или библиотеку функций, специфичных для проекта, над которым работает конкретный автор

## <a name="file">Файловая система, импорт/экспорт данных</a>

### Файловая система

`[x[0] for x in os.walk('my_directory')]`
> выводит список всех папок внутри папки 'my_directory' (включая подпапки)

### Импорт данных

`df = pd.read_csv('C:\\temp\\example.csv')`<br><br>
`df2 = pd.read_csv('C:\\temp\\example2.csv', parse_dates=['date'],  sep=';', dayfirst=True)`
> импорт с локального диска и сохранение файла в датафрейм, во втором примере указан парсинг дат, указан разделитель и формат времени dd/mm/yyyy

`connection_default = {'host': 'link',`<br>
`'database':'default',`<br>
`'user':'******',`<br>
`'password':'******'`<br>
`}`<br>

`q =`<br> 
```
    '''
    SELECT *
    FROM
        {db}.table
    WHERE
        id != 'none'
    ''' 
```
`df = ph.read_clickhouse(query=q, connection=connection_default)`
> SQL запрос в Python через CLICKHOUSE в пандас (нужно подставить link, заменить звездочки учетными данными, верно указать базу данных и написать корректный запрос)

`df = pd.read_csv('C:\\temp\\data.csv.zip',  compression='zip')`<br><br>
`with zipfile.ZipFile('C:\\temp\\data2.csv.zip') as myzip:`<br>
    `with myzip.open('data2.csv') as myfile:`<br>
        `df2 = pd.read_csv(myfile, encoding='ISO-8859-1')`
> импорт в датасет csv файла, а во втором случае импорт конкретного csv файла в архиве

`df = np.genfromtxt('C:\\temp\\data.txt', dtype=None)`<br>
`pd.DataFrame(df)`
> загружает данные из txt файла в виде массива numpy и создает датасет с этими данными

`df = pd.read_csv('C:\\temp\\data.txt', delimiter='\t', encoding='ansi')`<br>
> загружает табличные данные из txt файла и создает датасет с этими данными

`df = pd.read_excel('C:\\temp\\data.xls')`
> загружает данные из excel файла и создает датасет с этими данными (загружается первый лист по умолчанию)

`df = pd.read_excel('C:\\temp\\data.xls', sheet_name = 'Лист3')`
> загружает данные из excel файла c указанного листа

`df = pd.read_csv('link', sep=";")`
> загружает файл csv по ссылке

`def download(link, df):`<br>
    `   base_url = 'link'`<br>
    `   final_url = base_url + urlencode(dict(public_key=link))`<br>
    `   response = requests.get(final_url)`<br>
    `   download_url_file = response.json()['href']`<br>
    `   file_df = pd.read_csv(download_url_file)`<br>
    `   pattern = r'[;,|\t]'`<br>
    `   pattern_test = re.search(pattern, file_df.columns[0])`<br>
    `   if pattern_test is not None:`<br>
        `       sep = pattern_test[0]`<br>
        `       file_df = pd.read_csv(download_url_file, delimiter=None, sep=sep, parse_dates=True)`<br>
    `   globals()[df] = file_df`
> функция для загрузки файлов с ЯндексДиска (заполнить link)

### Экспорт данных

`filename = 'C:\\temp\\data.csv'<br>
`df.to_csv(filename)`
> сохраняет датасет в csv файл

`with pd.ExcelWriter('northwind_data.xlsx') as writer:`<br>
`   users_df.to_excel(writer, sheet_name='Users', index=False)`<br>
`   purchases_df.to_excel(writer, sheet_name='Purchases', index=False)`<br>
`   items_df.to_excel(writer, sheet_name='Items', index=False)`
> сохраняет датасеты в один excel файл, датасеты сохраняются на разные листы

## Условные выражения

`if a > b:`<br>
`    print(a)`<br>
`elif a == b:`<br>
`    print(0)`<br>
`else:`<br>
`    print(b)`<br>
> позволяет выполнить определенный набор инструкций в зависимости от некоторого условия>

`value = 20 if a >= 18 else 8`<br>
> оператор, который позволяет упростить запись условных выражений

`match value:`<br>
`    case <pattern_1>:`<br>
`        <action_1>`<br>
`    case <pattern_2>:`<br>
`        <action_2>`<br>
`    case <pattern_3>:`<br>
`        <action_3>`<br>
`    case _:`<br>
`        <action_wildcard>`<br>
> позволяет сопоставлять значения переменных с шаблонами и выполнять определенные действия в зависимости от соответствия шаблону

## <a name="string">Работа со строками</a>

`S1 + S2` 
>сложение строк

`S * 3`
повторение строки три раза

`S[i]`
> обращение по индексу символа в строке

`S[i:j:step]`
> позволяет выбрать срез последовательности элементов начиная с индекса i и заканчивая индексом j-1 с заданным шагом step

`len(S)`
> выводит длину строки

`S.find(str, [start],[end])`
> возвращает индекс первого символа первого вхождения str в S или -1, если подстрока не найдена.
> start является необязательным и указывает индекс, с которого нужно начать поиск подстроки. Если start не указан, поиск начинается с начала строки
> end также является необязательным и указывает индекс, до которого нужно искать подстроку. Если end не указан, поиск производится до конца строки

`S.rfind(str, [start],[end])`
> то же что и предыдущее, но возвращает номер последнего вхождения или -1

`S.index(str, [start],[end])`
> аналогично S.find, отличие в том, что если подстрока не найдена, S.index() генерирует исключение ValueError

`S.rindex(str, [start],[end])`
> то же что и предыдущее, но возвращает номер последнего вхождения или ValueError

`S.replace(шаблон, замена)`
> замена символов в строке по шаблону, можно указать третий необязательный параметр (число - какой по порядку элемент шаблона заменить)

`S.split(символ)`
> разбиение строки по разделителю, указанному в скобках

`S.isdigit()`
> cостоит ли строка из цифр - возвращает true или false

`S.isalpha()`
> cостоит ли строка из букв - возвращает true или false

`S.isalnum()`
> cостоит ли строка из цифр или букв - возвращает true или false

`S.islower()`
> cостоит ли строка из символов в нижнем регистре - возвращает true или false

`S.isupper()`
>cостоит ли строка из символов в верхнем регистре - возвращает true или false

`S.isspace()`
> cостоит ли строка из неотображаемых символов: пробел, перевод страницы новая строка и т.д. - возвращает true или false

`S.istitle()`
> начинаются ли слова в строке с заглавной буквы - возвращает true или false

`S.upper()`
> преобразовывает строку к верхнему регистру

`S.lower()`
> преобразовывает строку к нижнему регистру

`S.startswith(шаблон)`
> начинается ли строка с шаблона - возвращает true или false

`S.endswith(шаблон)`
> заканчивается ли строка шаблоном - возвращает true или false

`S.join(список)`
> объединяет элементы списка в одну строку S, используя строку S в качестве разделителя между элементами списка

`ord(символ)`
> возвращает код символа в таблице символов ASCII

`chr(число)`
> возвращает символ, соответствующий переданному числу в кодировке Unicode

`S.capitalize()`
> переводит первый символ строки в верхний регистр, а все остальные в нижний

`S.center(width, [fill])`
> выравнивает строку S по центру, путем добавления символов fill (если указано) слева и справа от строки до достижения заданной ширины (width)

`S.count(str, [start],[end])`
> возвращает количество непересекающихся вхождений подстроки в диапазоне [start, end]

`S.expandtabs([tabsize])`
> возвращает копию строки, в которой все символы табуляции заменяются одним или несколькими пробелами. Если TabSize не указан, табуляция полагается равным 8 пробелам

`S.lstrip([chars])`
> удаление пробельных символов в начале строки

`S.rstrip([chars])`
> удаление пробельных символов в конце строки

`S.strip([chars])`
> удаление пробельных символов в начале и в конце строки

`S.partition(шаблон)`
> возвращает кортеж из трех элементов: 1й будет содержать часть строки S, которая находится перед первым вхождением шаблона; 2й элемент будет содержать сам шаблон; 3й элемент будет содержать часть строки S, которая находится после первого вхождения шаблона

`S.rpartition(sep)`
> то же самое, что и предыдущая команда, но выполняется в обратном направлении

`S.swapcase()`
> переводит символы нижнего регистра в верхний, а верхнего – в нижний

`S.title()`
> первую букву каждого слова переводит в верхний регистр, а все остальные в нижний

`S.zfill(width)`
> используется для добавления ведущих нулей в строку S до заданной ширины width

`S.ljust(width, 'fillchar')`
> то же самое, что и предыдущая команда, но заполняет символом fillchar

`S.rjust(width, 'fillchar')`
> то же самое, что и предыдущая команда, но заполняет символом fillchar и выполняется в обратном направлении

`S.partition(sep)`
> разбивает строку по указанному разделителю и возвращает кортеж из трех элементов: строка до разделителя, сам разделитель и строка после разделителя

`S.rpartition(sep)`
> разбивает строку по последнему встреченному разделителю sep и возвращает кортеж, который состоит из трех элементов: строки до разделителя, самого разделителя и строки после разделителя

## <a name="list">Работа со списками</a>

`append()`
> добавляет в конец списка один новый элемент

`extend()`
> расширяет список другим списком

`insert(index, value)`
> позволяет вставлять значение в список в заданной позиции

`separator.join(list)`
> объединяет элементы списка в строку, разделенную указанным разделителем

`index()`
> возвращает индекс первого элемента, значение которого равняется переданному в метод значению

`remove()`
> удаляет первый элемент, значение которого равняется переданному в метод значению

`pop(index)`
> удаляет элемент по указанному индексу и возвращает его

`reverse()`
> меняет на противоположный порядок следования значений в списке

`len(list)`
> выводит количество элементов списка

`sum(list)`
> выводит сумму элементов в списке

`count()`
> возвращает количество элементов в списке, значения которых равны переданному в метод значению

`clear()`
> удаляет все элементы из списка

`copy()`
> создает поверхностную копию списка

`sort()`
> сортирует список, для обратного порядка указать reverse=True

## <a name="func">Списочные выражения и функции для работы с множествами</a>

`len`
> функция, которая возвращает количество элементов в последовательности

`max`
> функция, которая возвращает максимальное значение из последовательности

`min`
> функция, которая возвращает минимальное значение из последовательности

`sorted`
> функция, которая сортирует последовательность.
> Можно указать параметр reverse=True, чтобы отсортировать последовательность в обратном порядке

`range(start, stop, step)`
> позволяет нам генерировать последовательность целых чисел с заданным началом, концом и шагом

### Дополнительные примеры:
Дано:<br>
`word = 'test'`<br>
`numbers = [1, 5, 10, 25, 50]`<br>
`words = ['house', 'worm', 'sex']`<br>
`str_numbers` = '12345'`<br>
`mix = [1, 'house', 3.45]`

тогда:

| Выражение | Суть | Результат |
| ----------|------|-----------|
| [0 for i in range(10)]| создает список, содержащий определенное количество заданных элементов | [0, 0, 0, 0, 0] |
| [int(i) for i in str_numbers] | преобразует список строк из чисел в список целых чисел | [1, 2, 3, 4, 5] |
| [i ** 3 for i in range(1, 5)] | list comprehension, которое в данном случае генерит список кубов чисел in range | [1, 8, 27, 64] |
| [i for i in numbers if i != 10] | удаляет элементы из списка по условию | [1, 5, 25, 50] |
| [i * 2 for i in numbers]| создаст список, в котором каждый элемент "numbers" будет умножен на 2 | [2, 10, 20, 50, 100] |
|len([item for item in numbers if item > 5]) | вычисляет количество элементов списка numbers, которые больше 5 | 3 |
| [w * 2 for w in word] | создаст список с удвоенными значениями элементов word | ['tt', 'ee', 'ss', 'tt'] |
| [l[0] for l in words] | создаст список, содержащий первый элемент от каждого элемента списка words | ['h', 'w', 's'] |
| all(numbers[i] <= numbers[i+1] for i in range(len(numbers)-1)) | проверяет условие, что каждый последующий элемент списка "numbers" не меньше предыдущего элемента | True |
| [i for i in numbers if i < 15] | вернет список с элементами списка "numbers", которые меньше 15 | [1, 5, 10] |
| [l[0] for l in words if len(l) == 3] | вернет список из первых элементов каждого элемента списка words, длина которого равна 3 | ['s'] |

## <a name="set">Работа с множествами </a>

`|`
> объединение множеств

`&`
> пересечение множеств

`add()`
> добавляет элемент в множество

`union()`
> объединяет два множества

`clear()`
> удаляет все элементы из множества

`copy()`
> создает копию множества

`difference()`
> возвращает разность между двумя множествами

`intersection()`
> возвращает пересечение двух множеств

`isdisjoint()` 
> возвращает True, если два множества не имеют общих элементов

`issubset()`
> возвращает True, если одно множество является подмножеством другого

`discard(element)`
> удаляет элемент из множества, если он присутствует

`remove(element)`
> удаляет элемент из множества, если он присутствует, если отсутствует, то вызывается ошибка KeyError

### Работа с неизменяемыми структурами данных

`days_of_the_week = ('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday')`
> создание кортежа

`my_frozenset = frozenset([1, 2, 3])`
> создание неизменяемого множества

### Работа со словарями

`del dict["key"]`
> удаление элемента из словаря

`my_dict.setdefault('d', 4)`<br>
> возвращает значение по ключу, если ключ отсутствует, то он будет вставлен в словарь с указанным значением (или None, если значение не указано)

`dict1 = {'a': 10, 'b': 20}`<br>
`dict2 = {'b': 30, 'c': 40}`<br>
`dict1.update(dict2)`<br>
> объединяет словарь с другими словарями или ключ-значение парами, если ключ уже существует, то его значение будет обновлено

### Работа со временем

`df['column'] = pd.to_datetime(df.column)`
> преобразует столбец в формат даты и времени

`df['column'] = df['column'].dt.strftime('%m/%d/%Y')`
> преобразует значения столбца из формата datetime в строковый формат

`df['Date'].dt.day`
> извлечение из даты дня(если в столбце тип данных - дата), можно также day заменить на: week, month, quarter

### Прочие методы

Дано:<br>
`word = 'test'`<br>
`numbers = [1, 5, 10, 25, 50]`<br>
`words = ['house', 'worm', 'sex']`<br>
`str_numbers` = '12345'`<br>
`mix = [1, 'house', 3.45]`

`for num, word in zip(numbers, words):`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`print(f"{num} {word}")`<br>
`1 house`<br>
`5 worm`<br>
`10 sex`<br>
> использована функция zip() для создания итератора, который позволяет проходить по двум спискам одновременно

`result = 1`<br>
`for i in range(0,len(numbers)):`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`result = result * numbers[i]`<br>
`print(result)`<br>
`62500`
> перемножает все числа в списке "numbers" по очереди и сохраняет результат в переменной "result"

`print(list(map(type, mix)))`<br>
`[<class 'int'>, <class 'str'>, <class 'float'>]`
> выведет список, содержащий тип каждого элемента списка

`if len(set(numbers)) == 1:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`print('YES')`<br>
`else:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`print('NO')`<br>
`NO`<br>
> код проверяет, все ли элементы списка numbers равны между собой

`n = int(input())`<br>
`counter = []`<br>
`while n != 0:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`last_digit = n % 10`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`n = n // 10`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`counter.append(last_digit)`<br>
`counter.reverse()`<br>
`print(counter)`<br>
> выводит список цифр введенного пользователем числа в порядке от первой цифры до последней, и для числа 456 результатом будет [4, 5, 6]

## <a name="pandas">Работа с датафреймами (Pandas / NumPy)</a>

`pd.Series(['One','Two','Three'])`
> cоздает Series из списка

`df = pd.DataFrame()`
> cоздает пустой датафрейм

`df = df.reset_index()`
> возвращает датафрейм с новым индексом, начинающимся с 0, и старым индексом, сохраненным в виде нового столбца

`df['Column'].to_frame()`
> возвращает столбец из датафрейма в виде датафрейма

`df.isna().sum()`
> возвращает количество пропущенных значений в каждом столбце

`df.columns = [x.lower() for x in df.columns]`
> приводит все названия столбцов к нижнему регистру

`df.rename(columns={'x': 'name', 'y': 'new_name'}, inplace=True)`
> переименовывает столбцы в датасете

`df.rename(columns={df.columns[0]:"new_column_name"}, inplace=True)`
> переименовывает столбец в датасете по индексу

`df.reindex(columns=['column_1, 'column_2', 'column_3'], inplace=True)`
> меняет индекс столбцов в датасете

`df.columns = df.columns.str.replace('[.]', '_').str.replace('[ ]', '').lower()`
> переименовывает столбец в датасете, удаляя "лишние" символы и переводит к нижнему регистру

`df[['One', 'Two', 'Three']]`
> выводит три столбца датафрейма df в виде датафрейма

`df = df.drop(['column_1','column_2'], axis=1)`<br>
`df = df.loc[:,('column_1', 'column_4', 'column_7')]`<br>
`df = df.drop(columns=df.columns[0])`
> удаление столбцов в датасете:<br> первый - удаляет указанные столбцы;<br> второй - удаляет все столбцы, кроме указанных;<br> третий - удаляет столбцы по индексу

`df['numbers'] = df['numbers'].astype(int)`<br>
`df = df.apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()`
> первый код переводит текстовые значения в целые числа,<br>
> второй код позволяет также удалить все строки с некорректными или отсутствующими значениями - оставить только строки с числовыми значениями

`len(df.loc[df.assign_type != df.reserved_type])`
> позволяет вывести количество строк не соответствующих строкам другого столбца

`df.groupby('year').month.agg(pd.Series.mode)`
> группирует строки по году и для каждой группы найдет моду (самое часто встречающееся значение), в данном случае, месяца

`df = df.astype({"a": int, "b": complex})`
> меняет типы данных в столбцах

`df['column'].tolist()`
> переводит cтолбец колонки в список

`df = df['column'] + '\\' + df['column_2']`
> делает из содержимого двух колонок третью

`df = df.assign(new_column = df.column_1 * df.column_2)`
> cоздает столбец на основании математической операции между двумя столбцами

`orders.isna().sum()`
> выводит информацию по пропущенным значениям в датасете

`df.groupby('column_1').agg({'success': 'sum'})`
> группирует датафрейм по количество успешных операций для столбца column_1

`df = df.unstack('column_1')`
> преобразует иерархически индексированный датасет, добавляя колонки из уровня, соответствующего указанной колонке, на новый уровень колонок

`df2 = pd.wide_to_long(df, ['column_1', 'column_2'], i=['type', 'name'], j='year', sep='_')`
> создает новый датасет, в котором идентификаторы (в данном случае, type, name) хранятся в отдельных столбцах, а значения из исходных столбцов с годами (year) и значениями (column_1, column_2) хранятся в отдельных строках

`df2 = df.explode('Column_1')`
> создает новый датасет, в котором каждый элемент списка в столбце 'Column_1' разбивается на отдельную строку, а значения в остальных столбцах дублируются соответственно

`df.groupby('column_1').agg({'column_2': 'sum'})`
> группирует строки в датасете df по уникальным значениям в столбце "column_1" и для каждой группы вычисляет сумму значений в столбце "column_2". Результатом будет новый датасет с одним индексом "column_1" и одним столбцом "column_2", содержащим сумму значений "column_2" для каждого уникального значения "column_1"

`df.sort_values('column_2', ascending=False)`
> сортирует датасет df по столбцу column_2 в порядке убывания (от наибольшего значения к наименьшему)

`df.sort_values(['Column_1','Column_2'], ascending = [True, False])`
> сортитует датасет сразу по двум колонкам, одну по убыванию, другую по возрастанию

`df['name'].value_counts()`
> используется для подсчета количества уникальных значений в столбце "name" датасета df и создания объекта типа Series с количеством вхождений каждого уникального значения

#### Join датасетов

`df_merged = pd.merge(df_1, df_2[['column_1', 'column_2']], left_on=['column_1'], right_on=['column'], how='left')`
> соединяет два датасета, используя левое соединение, из "правой" таблицы берутся только указанные столбцы

#### Фильтрация данных в датасетах

`df = df.drop_duplicates()`
> удаляет дубликаты

`df = df.sort_values('номер').drop_duplicates(subset=df.columns.difference(['номер']), keep='first')`
> удаляет дубликаты, если отличаются значения только в одном столбце - оставляется наименьшее значение по данному столбцу

`df.query("name == 'John' and surname == 'Doe'")`
> выбирает в датасете df все строки, где значение столбца "name" равно "John" и значение столбца "surname" равно "Doe"

`df[df['price']>500]`
> выводит строки датасета, где значение столбца "price" больше 500

`df[~(df['price']>500)]`
> выводит строки датасета, где значение столбца "price" меньше 500

`df[df['price']>500][['product_id', 'weight']]`
> выводит после фильтрации только два указанных столбца в отфильтрованном датасете

`df[df['price'].between(10,20)]`
> выводит строки датасета, где значение столбца "price" между 10 и 20 (включительно)<br>
> можно добавить параметр inclusive со следующими значениями: 'neither' - не захватывает границы диапазона;'left' - захватывает левую границу диапазона; 'right' - захватывает правую границу диапазона

`df[df['Title'].isin(['Sales Agent','Sales Manager','Assistant Sales Agent'])]`
> выводит строки датасета, в которых по столбцу Title указаны только значения из списка

`df[(df['price']>500)&(df['OrderDate'].between('1997-01-01', '1997-12-31'))]`
> пример объединения нескольких фильтров с оператором and. Объединять можно также с оператором or, который прописывается таким символом: |

`df[df['Column'].str.startswith('A')]`
> выводит датасет со строками начинающимися с символа A. В методе можно применять иные команды работы со строками: contains, endswith и т.д.

#### Описательная статистика датасетов

`df.info()`
>  позволяет получить общую информацию о датасете, такую как количество строк, количество столбцов, их имена и типы данных, количество ненулевых значений в каждом столбце и использованную память

`df.dtypes`
>  позволяет получить информацию о типах данных столбцов

`df.groupby(['name', 'age']).agg({'income': 'describe'})`
> вычисляет основные статистические показатели для каждой группы значений столбца income, включая количество наблюдений (count), среднее значение (mean), стандартное отклонение (std), минимальное и максимальное значения (min и max) и квартили (25%, 50%, 75%)

`df.describe(include='object')`<br>
`df.describe(include='datetime')`
> первый используется для получения статистической информации о колонках в объекте Pandas DataFrame, содержащих данные типа "object", второй для для колонок даты и времени

## Графики в Python

#### Визуализация данных
`data = {'samples': [1, 2, 3, 4, 5], 'values': [10, 15, 20, 25, 30]}`<br>
`df = pd.DataFrame(data)`
> данные для визуализации

##### Гистограмма
`import matplotlib.pyplot as plt`<br>
`plt.hist(df['samples'], bins=5, weights=df['values'])`<br>
`plt.xlabel('Значение')`<br>
`plt.ylabel('Частота')`<br>
`plt.show()`

#### Визуализация проверки нормальности распределения
`data = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 5, 5, 5, 5, 6, 7, 9, 10, 11]`<br>
> данные для визуализации

##### QQ-график

`import numpy as np`<br>
`import statsmodels.api as sm`<br>
`import matplotlib.pyplot as plt`<br>
`sm.qqplot(np.array(data), line ='45')`<br>
`plt.show()`<br>
> график используется для визуальной оценки соответствия распределения данных нормальному распределению. Он позволяет сравнить квантили выборки с квантилями нормального распределения

##### Гистограмма

`import matplotlib.pyplot as plt`<br>
`plt.hist(data, bins=10)`<br>
`plt.title('Гистограмма распределения данных')`<br>
`plt.xlabel('Значения')`<br>
`plt.ylabel('Частота')`<br>
`plt.show()`
> показывает частоту появления различных значений в выборке. Гистограмма может помочь оценить форму распределения данных

##### Ядерная оценка плотности

`import seaborn as sns`<br>
`import matplotlib.pyplot as plt`<br>
`sns.kdeplot(data, shade=True)`<br>
`plt.show()`<br>
> график представляет собой сглаженную оценку плотности вероятности распределения данных. Он также может помочь визуально оценить форму распределения

##### Box plot

`import matplotlib.pyplot as plt`<br>
`fig, ax = plt.subplots()`<br>
`ax.boxplot(data)`<br>
`ax.set_title('Box plot')`<br>
`ax.set_xlabel('Data')`<br>
`ax.set_ylabel('Values')`<br>
> позволяет оценить медиану, квартили и размах данных, что может быть полезно при анализе нормальности распределения

## Методы теории вероятностей в Python

##### Формула Бернулли

`import math`<br>
`def calculate_probability(n, k, p):`<br>
    `    # Биномиальный коэффициент`<br>
    `    binomial_coefficient = math.comb(n, k)`<br>
    `    # Вероятность наступления события`<br>
    `    probability = binomial_coefficient * (p ** k) * ((1 - p) ** (n - k))`<br>
    `    return probability`<br>
`# Запрос данных у пользователя`<br>
`n = int(input("Введите число испытаний (n): "))`<br>
`k = int(input("Введите количество интересующих испытаний (k): "))`<br>
`p = float(input("Введите вероятность интересующего испытания (p): "))`<br>
`# Вычисление вероятности`<br>
`result = calculate_probability(n, k, p)`<br>
`print(f"Вероятность наступления события: {result}")`<br>
> решает задачу на определение вероятности наступления события при определенном количестве наблюдений

`import math`<br>
`def calculate_probability_at_least(n, k, p):`<br>
    `    probability = 0`<br>
    `    for i in range(k, n+1):`<br>
    `        binomial_coefficient = math.comb(n, i)`<br>
    `        probability += binomial_coefficient * (p ** i) * ((1 - p) ** (n - i))`<br>
    `    return probability`<br>
`# Запрос данных у пользователя`<br>
`n = int(input("Введите число испытаний (n): "))`<br>
`k = int(input("Введите количество интересующих испытаний (k): "))`<br>
`p = float(input("Введите вероятность интересующего испытания (p): "))`<br>
`# Вычисление вероятности`<br>
`result = calculate_probability_at_least(n, k, p)`<br>
`print(f"Вероятность наступления события: {result:.3f}")`<br>
> решает задачу на определение вероятности того, что событие наступит хотя бы k раз из n испытаний с помощью биномиального распределения

## <a name="math">Математические методы в Python</a>

> Для исполнения кодов раздела необходимо импортировать библиотеки/модули:<br>
> math;<br>

##### Простые операции

`pow(2, 3)`
> возведение первого числа в степень второго

`abs()`
> находит модуль числа

`9**(1/2) = 9**0.5 = 3.0`
> находит корень из числа

##### Логарифмы

###### Натуральный логарифм
`ln_x = math.log(x)`

###### Десятичный логарифм
`ln_10_x = math.log10(x)`

###### Экспонента
`exp_x = math.exp(x)`

###### Логарифмирование значений столбца датасета:
`df = np.log(df.view)`

##### Среднее
`df_mean = df.mean()`

###### Скользящее среднее
`df_rolling_mean = df.rolling(window=2).mean()`

###### Экспоненциальное скользящее среднее:
`df_ewm_rolling_mean = df.ewm(span=2).mean()`

##### Исчисление процентов

###### Процентное отношение значений ячеек столбца к общему значению
`df["percent"] = (df["users"] / df["users"].sum()) * 100`

## <a name="statistics">Статистические методы в Python</a>

Содержание:<br>
- [Меры центральной тенденции](#mct)<br>
- [Меры изменчивости](#mch)<br>
- [Меры статистической связи между переменными](#con)<br>
- [Z-преобразование](#z)<br>
- [Проверка распределения на нормальность](#norm)<br>
- [Проверка гомогенности дисперсий](#hom)<br>
- [Определение доверительных интервалов](#hon)<br>
- [Статистические тесты (критерии)](#tests)<br>

> Для исполнения кодов раздела необходимо импортировать библиотеки/модули:<br>
> statistics, numpy;<br>
> stats from scipy;<br>
> normaltest, shapiro, normaltest, levene, bartlett, norm, ttest_ind, chi2, chi2_contingency from scipy.stats;<br>
> math;<br>
> pingouin;<br>

<a id='mct'></a>
#### Меры центральной тенденции

###### Мода

`data = [1, 2, 2, 3, 4, 4, 4]`<br>
`print("Мода: ", statistics.mode(data))`<br>
`Мода:  4`<br>
> Мода - это значение в наборе данных, которое встречается наиболее часто

###### Среднее

`data = [1, 2, 2, 3, 4, 4, 4]`<br>
`print("Среднее: ", statistics.mean(data))`<br>
`Среднее:  2.857142857142857`<br>
> Среднее значение, также известное как среднее арифметическое, вычисляется путем сложения всех значений в наборе данных и деления полученной суммы на количество значений в наборе<br>
> $$\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i$$<br>
> где $\bar{x}$ обозначает среднее значение, $n$ - количество элементов в выборке, а $x_1, x_2, ..., x_n$ - значения элементов выборки. Формула использует знак суммы $\sum$, который означает суммирование всех значений $x$ в выборке.

###### Медиана

`data = [1, 2, 2, 3, 4, 4, 4]`<br>
`print("Медиана: ", statistics.median(data))`<br>
`Медиана:  3`<br>
> Значение, которое разделяет упорядоченный набор данных на две равные половины
> Формула для расчета с четным числом элементов выборки:
> $$median=\frac{x_{\frac{n}{2}} + x_{\frac{n}{2}+1}}{2}$$
> где где $n$ - количество элементов в выборке, $x_{\frac{n}{2}}$ - значение элемента с индексом $\frac{n}{2}$ и $x_{\frac{n}{2}+1}$ - значение элемента с индексом $\frac{n}{2}+1$
> Формула для расчета с нечетным числом элементов выборки:
> $$median = x_{\frac{n+1}{2}}$$
> где $x$ - массив элементов, а $n$ - число элементов в массиве

###### Межквартильный размах

`data = np.array([12, 15, 16, 18, 19, 20, 22, 25, 28, 31, 35])`<br>
`Q1 = np.percentile(data, 25, interpolation='midpoint')`<br>
`Q3 = np.percentile(data, 75, interpolation='midpoint')`<br>
`IQR = Q3 - Q1`<br>
`print("Межквартильный размах:", IQR)`<br>
`Межквартильный размах: 9.5`
> Межквартильный размах - определяется как разница между верхним и нижним квартилями в наборе данных.
> Квартили - это значения, которые разделяют упорядоченный набор данных на четыре равные части. Верхний квартиль (Q3) - это значение, которое делит верхние 25% данных от нижних 75%, а нижний квартиль (Q1) - это значение, которое делит нижние 25% данных от верхних 75%.
> Таким образом, межквартильный размах (IQR) - это разница между Q3 и Q1. Он представляет собой интерквартильный диапазон, который содержит 50% данных и позволяет оценить разброс значений в наборе данных, игнорируя выбросы

<a id='mch'></a>
#### Меры изменчивости

###### Дисперсия

`data = [1, 5, 2, 7, 1, 9, 3, 8, 5, 9]`<br>
`print("Дисперсия: ", statistics.variance(data))`<br>
`Дисперсия:  10`
> Дисперсия - это среднее арифметическое квадратов отклонений каждого элемента выборки от ее среднего значения. Она измеряет, насколько значения в выборке отклоняются от ее среднего значения и показывает, как распределены значения вокруг среднего значения. Дисперсия выражается в квадратных единицах измерения выборки
> $$s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2$$
> где $s^2$ - выборочная дисперсия, $n$ - размер выборки, $x_i$ - i-й элемент выборки, $\bar{x}$ - выборочное среднее.

###### Стандартное отклонение

`data = [1, 5, 2, 7, 1, 9, 3, 8, 5, 9]`<br>
`print("Стандартное отклонение: ", statistics.stdev(data))`<br>
`Стандартное отклонение:  3.1622776601683795`
> Стандартное отклонение - это квадратный корень из дисперсии выборки. Оно измеряет степень разброса данных относительно их среднего значения и представляет собой стандартную меру распределения выборки. Стандартное отклонение может быть выражено в тех же единицах измерения, что и сама выборка.
> Таким образом, стандартное отклонение и дисперсия оба показывают, насколько значения в выборке отклоняются от ее среднего значения, но стандартное отклонение также показывает, как распределены значения вокруг среднего значения.
> $$\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})^2}$$
> где $N$ - количество значений, $x_i$ - i-е значение, а $\bar{x}$ - среднее значение

###### Среднеквадратическое отклонение
`data = [1, 5, 2, 7, 1, 9, 3, 8, 5, 9]`<br>
`mean_value = statistics.mean(data)`<br>
`print("Среднеквадратическое отклонение: ", statistics.stdev(data, xbar=mean_value))`<br>
`Среднеквадратическое отклонение:  3.1622776601683795`
> Среднеквадратическое отклонение - это корень из среднего арифметического квадратов отклонений каждого элемента выборки от ее среднего значения. Оно также измеряет степень разброса данных относительно их среднего значения и представляет собой стандартную меру распределения выборки. Среднеквадратическое отклонение может быть выражено в тех же единицах измерения, что и сама выборка.
> $$\sigma = \sqrt{\frac{1}{N}\sum_{i=1}^{N}(x_i - \mu)^2}$$
> где $N$ - количество элементов в выборке, $x_i$ - i-й элемент выборки, а $\mu$ - среднее значение выборки

###### Стандартная ошибка среднего
`variance = 4`<br>
`n = 100`<br>
`SEM = math.sqrt(variance/n)`<br>
`print("SEM:", SEM)`<br>
`SEM: 0.2`
> Стандартная ошибка среднего - мера разброса среднего значения в выборке относительно среднего значения в генеральной совокупности, это среднеквадратическое отклонение распределения выборочных средних
> $$SE = \frac{s}{\sqrt{n}}$$
> где $s$ - стандартное отклонение выборки, $n$ - размер выборки

<a id='con'></a>
#### Меры статистической связи между переменными

###### Коэффициент корреляции

`x = np.array([1, 2, 3, 4, 5])`<br>
`y = np.array([5, 7, 9, 11, 13])`<br>
`corr_coef = np.corrcoef(x, y)[0, 1]`<br>
`print("Коэффициент корреляции между x и y:", corr_coef)`<br>
`Коэффициент корреляции между x и y: 0.9999999999999999`
> Это мера статистической связи между двумя переменными, которая показывает, насколько сильно связаны эти переменные между собой. Положительный коэффициент корреляции указывает на прямую связь между переменными. Отрицательный коэффициент корреляции указывает на обратную связь между переменными. Коэффициент корреляции, равный (или близкий) нулю, означает отсутствие связи между переменными
> $$r = \frac{n\sum_{i=1}^{n}x_iy_i - \sum_{i=1}^{n}x_i\sum_{i=1}^{n}y_i}{\sqrt{\left[n\sum_{i=1}^{n}x_i^2 - \left(\sum_{i=1}^{n}x_i\right)^2\right]\left[n\sum_{i=1}^{n}y_i^2 - \left(\sum_{i=1}^{n}y_i\right)^2\right]}}$$
> где $n$ - размер выборки, $x_i$ и $y_i$ - соответствующие значения в выборках X и Y

###### Коэффициент детерминации

`x = np.array([1, 2, 3, 4, 5])`<br>
`y = np.array([5, 7, 9, 11, 13])`<br>
`# оценка параметров линейной регрессии`<br>
`slope, intercept = np.polyfit(x, y, 1)`<br>
`# расчет коэффициента детерминации`<br>
`y_predicted = slope * x + intercept`<br>
`r_squared = 1 - (np.sum((y - y_predicted) ** 2) / ((len(y) - 1) * np.var(y, ddof=1)))`<br>
`print("Коэффициент детерминации:", r_squared)`<br>
`Коэффициент детерминации: 1.0`
> (R-квадрат) - это мера, которая показывает, какую долю изменчивости одной переменной можно объяснить вариацией другой переменной. Значение R-квадрат находится в диапазоне от 0 до 1, где 0 означает, что модель не объясняет никакой изменчивости, а 1 означает, что модель объясняет всю изменчивость
> $$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$
> где $SS_{res}$ - остаточная сумма квадратов, а $SS_{tot}$ - общая сумма квадратов

<a id='z'></a>
#### Z-преобразование (стандартизация)

###### z-оценка
`zscore(df.column)`
> мера того, насколько значение точки данных отклоняется от среднего значения в единицах стандартного отклонения,
> это позволяет сравнивать данные, измеренные в разных единицах или имеющие разные масштабы
> $$z = \frac{X - \mu}{\sigma}$$
> где $z$ - z-оценка, $X$ - одно необработанное значение данных, $\mu$ - среднее значение набора данных, $\sigma$ - стандартное отклонение набора данных

###### Для генеральной совокупности
`data = np.array([12, 15, 16, 18, 19, 20, 22, 25, 28, 31, 35])`<br>
`mean = np.mean(data)`<br>
`std = np.std(data)`<br>
`z_data = stats.zscore(data)`<br>
`print('Z-преобразованные данные:', z_data)`<br>
`Z-преобразованные данные: [-1.45683394 -1.01577412 -0.86875418 -0.57471431 -0.42769437 -0.28067443
  0.01336545  0.45442527  0.89548508  1.3365449   1.92462466]`
  > Z-преобразование - это процесс стандартизации набора данных путем вычитания среднего значения и деления на стандартное отклонение. Если результат z-преобразования равен 0, то это означает, что значение переменной равно ее среднему значению. Если результат равен 1, то это означает, что значение переменной отклоняется от ее среднего значения на одно стандартное отклонение
  > $$Z = \frac{x - \mu}{\sigma}$$
  > где - $x$ - наблюдаемое значение, $\mu$ - среднее значение распределения, $\sigma$ - стандартное отклонение распределения

###### Для выборки из генеральной совокупности
`all_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])`<br>
`sample = np.random.choice(all_data, size=5, replace=False)`<br>
`sample_mean = np.mean(sample)`<br>
`sample_std = np.std(sample, ddof=1)`<br>
`z_sample = (sample - sample_mean) / sample_std`<br>
`print('Z-преобразованная выборка:', z_sample)`<br>
`Z-преобразованная выборка: [-1.19571306 -0.15596257  1.14372554  0.88378792 -0.67583782]`
> для применения z-преобразования к выборке необходимо вычислить выборочное среднее значение и выборочное стандартное отклонение
> $$Z = \frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}}$$
> где $\bar{x}$ - выборочное среднее, $\mu$ - среднее значение генеральной совокупности, $\sigma$ - стандартное отклонение генеральной совокупности, а $n$ - размер выборки

###### Для отдельного наблюдения из выборки
`sample = np.array([4, 5, 6, 7, 8, 9, 10])`<br>
`sample_mean = np.mean(sample)`<br>
`sample_std = np.std(sample, ddof=1)`<br>
`z_sample = (sample[4] - sample_mean) / sample_std`<br>
`print('Отдельно взятое наблюдение выборки:', sample[4])`<br>
`print('Z-значение для отдельно взятого наблюдения:', z_sample)`<br>
`Отдельно взятое наблюдение выборки: 8`<br>
`Z-значение для отдельно взятого наблюдения: 0.4629100498862757`<br>
> $$z = \frac{x - \mu}{\sigma}$$
> где $x$ - значение в выборке, $\mu$ - среднее значение выборки, $\sigma$ - стандартное отклонение выборки

<a id='norm'></a>
#### Проверка распределения на нормальность

> ни один из этих тестов не гарантирует, что данные точно имеют нормальное распределение. Они только дают представление о том, насколько близки данные к нормальному распределению

###### Тест Шапиро-Уилка

`data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`<br>
`stat, p = shapiro(data)`<br>
`print('Statistics=%.3f, p=%.3f' % (stat, p))`<br>
`alpha = 0.05`<br>
`if p > alpha:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`print('Распределение похоже на нормальное (не отвергаем H0)')`<br>
`else:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`print('Распределение не похоже на нормальное (отвергаем H0)')`<br>
`Statistics=0.970, p=0.892`<br>
`Распределение похоже на нормальное (не отвергаем H0)`
> Тест Шапиро-Уилка является более чувствительным к отклонениям от нормальности, особенно в случаях, когда данные имеют ярко выраженные выбросы. Он также более точен при малых выборках. Однако, он может оказаться менее мощным в случае, когда выборка слишком большая

###### Тест хи-квадрат

`data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`<br>
`stat, p = normaltest(data)`<br>
`print('Statistics=%.3f, p=%.3f' % (stat, p))`<br>
`alpha = 0.05`<br>
`if p > alpha:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`print('Распределение похоже на нормальное (не отвергаем H0)')`<br>
`else:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`print('Распределение не похоже на нормальное (отвергаем H0)')`<br>
`Statistics=2.027, p=0.363`<br>
`Распределение похоже на нормальное (не отвергаем H0)`
> Тест хи-квадрат может быть более устойчивым в случае, когда данные более-менее нормальные, но не слишком точны. Также, он может быть полезен, когда нам интересно не только проверить, нормальные ли данные, но и определить, насколько сильно они отклоняются от нормальности

###### qq-plot

`pg.qqplot(df, dist='norm')`
> график используется для сравнения распределения выборки данных df с теоретическим нормальным распределением

<a id='hom'></a>
#### Проверка гомогенности дисперсий

> Тесты Левена и Бартлетта применяются для проверки равенства дисперсий в нескольких выборках

###### Тест Левена

`data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]`<br>
`statistic, pvalue = levene(*data)`<br>
`if pvalue > 0.05:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`print(f"p-value = {pvalue}. Гомогенность дисперсий присутствует")`<br>
`else:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`print(f"p-value = {pvalue}. Гомогенность дисперсий отсутствует")`<br>
`p-value = 1.0. Гомогенность дисперсий присутствует`
> Тест основан на сравнении среднеквадратических отклонений (MSE) в группах. Он менее чувствителен к выбросам, чем тест Бартлетта, так как MSE рассчитывается на основе медианы, а не на основе среднего значения. Он предполагает нормальность распределения в каждой группе. Если распределение не является нормальным, то тест Левена может давать неправильные результаты
> $$W=\frac{(N-k) \sum_{i=1}^{k}\left(n_{i}\left(Z_{i}-\overline{Z}\right)^{2}\right)}{(k-1) \sum_{i=1}^{N}\left(Y_{i}-\overline{Y}\right)^{2}}$$
> где $W$ - статистика теста Левена, $N$ - общее число наблюдений, $k$ - количество групп, $n_i$ - размер $i$-й группы, $Z_i$ - среднее значение $i$-й группы, $\overline{Z}$ - общее среднее значение всех групп, $Y_i$ - каждое наблюдение в $i$-й группе, $\overline{Y}$ - общее среднее значение всех наблюдений

> или можно сделать тоже самое иным способом:<br>
`pg.homoscedasticity(df, dv='events', group='group')`<br>
> 'df' - аргумент, датасет, в котором должны содержаться данные для анализа на гомоскедастичность;<br>
> "dv='events'" - переменная (колонка), c зависимой переменной (в данном случае 'events');<br>
> "group='group'" - переменная (колонка), c независимой переменной и разделяет данные на группы (в данном случае 'group')

###### Тест Бартлетта

`group1 = [1, 2, 3, 4, 5]`<br>
`group2 = [2, 4, 6, 8, 10]`<br>
`stat, p = bartlett(group1, group2)`<br>
`if p < 0.05:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`result = 'Отвергаем гипотезу о гомогенности дисперсий'`<br>
`else:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`result = 'Не отвергаем гипотезу о гомогенности дисперсий'`<br>
`print(f'Тест Бартлетта: статистика = {stat:.3f}, p-значение = {p:.3f}. {result}')`<br>
`Тест Бартлетта: статистика = 1.587, p-значение = 0.208. Не отвергаем гипотезу о гомогенности дисперсий`
> Тест основан на сравнении дисперсий в группах. Он более чувствителен к выбросам, чем тест Левена, так как рассчитывает статистику на основе среднего значения, а не на основе медианы. Он предполагает нормальность распределения в каждой группе. Если распределение не является нормальным, то тест Бартлетта может давать неправильные результаты
> $$B = \frac{(N-1)\sum_{i=1}^k n_i \ln(s_i^2) - N \ln(s_p^2)}{1 + \frac{1}{3(k-1)}\left(\sum_{i=1}^k \frac{1}{n_i} - \frac{1}{N}\right)}$$
> где $N$ - общее количество наблюдений, $k$ - количество групп, $n_i$ - количество наблюдений в $i$-й группе, $s_i^2$ - выборочная дисперсия $i$-й группы, $s_p^2$ - средняя выборочная дисперсия всех групп

> или можно сделать тоже самое иным способом:<br>
`pg.homoscedasticity(df, dv='weight', group='group', method='bartlett')`<br>
> 'df' - аргумент, датасет, в котором должны содержаться данные для анализа на гомоскедастичность;<br>
> "dv='weight'" - зависимая переменная, которую нужно проверить на гомоскедастичность;<br>
> "group='group'" - независимая переменная, определяющая группы, между которыми сравниваются дисперсии;<br>
> "method='bartlett'" - метод, который будет использоваться для проверки гомоскедастичности

<a id='hon'></a>
#### Определение доверительных интервалов

`n = 30          # размер выборки`<br>
`x_bar = 7.5     # выборочное среднее`<br>
`alpha = 0.05    # уровень доверия`<br>
`# известное стандартное отклонение генеральной совокупности`<br>
`sigma = 2.5`<br>
`# находим значение квантиля распределения`<br>
`z = norm.ppf(1 - alpha/2)`<br>
`# вычисляем границы доверительного интервала`<br>
`lower = x_bar - z * sigma / math.sqrt(n)`<br>
`upper = x_bar + z * sigma / math.sqrt(n)`<br>
`print(f'Доверительный интервал: [{lower:.3f}, {upper:.3f}]')`<br>
`Доверительный интервал: [6.605, 8.395]`
> Доверительный интервал - это диапазон значений, в котором находится неизвестный параметр генеральной совокупности с известным уровнем доверия
> $$\bar{x} \pm z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}$$
> где $\bar{x}$ - выборочное среднее, $z_{\alpha/2}$ - критическое значение стандартного нормального распределения, соответствующее уровню доверия $\alpha$, $\sigma$ - известное стандартное отклонение, $n$ - размер выборки

###### Основываясь на знании t - распределения

`# устанавливаем уровень доверия`<br>
`confidence_level = 0.95`<br>
`# задаем количество степеней свободы`<br>
`degrees_of_freedom = 19`<br>
`# вычисляем критическое значение t-статистики`<br>
`t_critical = stats.t.ppf((1 + confidence_level) / 2, degrees_of_freedom)`<br>
`# задаем значения стандартного отклонения, размера выборки и стандартной ошибки среднего`<br>
`sd = 11.3`<br>
`n = 20`<br>
`standard_error = sd / np.sqrt(n)`<br>
`# задаем выборочное среднее и предельную ошибку`<br>
`sample_mean = 89.9`<br>
`margin_of_error = t_critical * standard_error`<br>
`# рассчитываем доверительный интервал`<br>
`confidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)`<br>
`print("Доверительный интервал для t - распределения: ", confidence_interval)`<br>
`Доверительный интервал для t - распределения:  (84.61143720745503, 95.18856279254499)`
> $$CI = \bar{x} \pm t_{\frac{\alpha}{2},n-1}\frac{s}{\sqrt{n}}$$
> где $\bar{x}$ - среднее значение выборки, $t_{\frac{\alpha}{2},n-1}$ - критическое значение t-распределения с $\frac{\alpha}{2}$ уровнем значимости и $n-1$ степенями свободы (где $\alpha$ - уровень значимости), $s$ - стандартное отклонение выборки, $n$ - размер выборки

<a id='tests'></a>
#### Статистические тесты (критерии)

###### Двухвыборочный t-критерий Стьюдента

`group1 = [3, 5, 7, 9, 11]`<br>
`group2 = [2, 4, 6, 8, 10]`<br>
`# вычисляем средние значения`<br>
`mean1 = sum(group1) / len(group1)`<br>
`mean2 = sum(group2) / len(group2)`<br>
`# вычисляем стандартные отклонения`<br>
`std1 = stats.tstd(group1)`<br>
`std2 = stats.tstd(group2)`<br>
`# вычисляем t-статистику`<br>
`t = (mean1 - mean2) / ((std1 ** 2 / len(group1)) + (std2 ** 2 / len(group2))) ** 0.5`<br>
`# определяем степени свободы`<br>
`df = len(group1) + len(group2) - 2`<br>
`# определяем p-уровень значимости`<br>
`p = stats.t.sf(abs(t), df) * 2`<br>
`# задаем уровень значимости`<br>
`alpha = 0.05`<br>
`if p < alpha:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`print(f"Различия статистически значимы: p={p}")`<br>
`else:`<br>
&nbsp;&nbsp;&nbsp;&nbsp;`print(f"Различия не являются статистически значимыми:p={p}")`<br>
`Различия не являются статистически значимыми:p=0.6305360755569764`
> тест, используемый для проверки значимости различий между двумя выборками. В основе t-критерия лежит понятие t-статистики, которая представляет собой отношение разности между средними значениями двух выборок к их стандартной ошибке. Важным условием применения t-критерия является то, что данные должны быть распределены нормально или близко к нормальному распределению. Если это условие не выполняется, то использование t-критерия может привести к неверным результатам
> $$t = \frac{\overline{X_1} - \overline{X_2}}{s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$$
> где $\overline{X_1}$ и $\overline{X_2}$ - выборочные средние первой и второй выборок, $s_p$ - выборочное стандартное отклонение, рассчитанное по объединенным выборкам, $n_1$ и $n_2$ - размерности первой и второй выборок соответственно

###### Распределение Хи-квадрат (Chi-squared) Пирсона для одной номинативной переменной

<pre>
<em># Наблюдаемые частоты</em>
observed = np.array([10, 10, 10, 5, 10, 15])
<em># Ожидаемые частоты (равномерное распределение)</em>
expected_uniform = np.array([10, 10, 10, 10, 10, 10])
<em># Вычисляем статистику Хи-квадрат</em>
chi2_stat = np.sum((observed - expected_uniform)**2 / expected_uniform)
dof = len(observed) - 1
<em># Вычисляем P-значение</em>
p_value = 1 - chi2.cdf(chi2_stat, dof)
<em># Задаем пороговый уровень значимости</em>
alpha = 0.05
print("Статистика Хи-квадрат:", chi2_stat)
print("P-значение:", p_value)
print("Степени свободы:", dof)
print("\nГипотезы:")
print("H0: эмпирическое распределение частот не отличается от равномерного")
print("H1: эмпирическое распределение частот отличается от равномерного")
print("\nРезультат:")
if p_value < alpha:
    print(f"Отвергаем H0 на уровне значимости {alpha}. Эмпирическое распределение частот отличается от равномерного.")
else:
    print(f"Не можем отвергнуть H0 на уровне значимости {alpha}. Эмпирическое распределение частот не отличается от равномерного.")
</pre>
Вывод:

<pre>
Статистика Хи-квадрат: 5.0
P-значение: 0.415880186995508
Степени свободы: 5

Гипотезы:
H0: эмпирическое распределение частот не отличается от равномерного
H1: эмпирическое распределение частот отличается от равномерного

Результат:
Не можем отвергнуть H0 на уровне значимости 0.05. Эмпирическое распределение частот не отличается от равномерного.
</pre>
>  Это статистический тест, который позволяет оценить, есть ли значимая связь между двумя категориальными переменными на основе наблюдаемых частот в таблице сопряженности<br>
> Критерий Хи-квадрат Пирсона сравнивает наблюдаемые частоты с частотами, которые ожидались бы при условии независимости переменных (то есть, при отсутствии связи между ними). Статистика Хи-квадрат Пирсона (χ²) является мерой расхождения между наблюдаемыми и ожидаемыми частотами и имеет распределение Хи-квадрат с определенным числом степеней свободы
> $$\chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$
> где $\sum_{i=1}^{r}$ и $\sum_{j=1}^{c}$ - сумма по всем строкам, а $O_{ij}$ — наблюдаемые частоты  в ячейках таблицы, $E_{ij}$ — ожидаемые частоты в ячейках, $r$ — количество строк таблицы сопряженности, $c$ — количество столбцов таблицы сопряженности

###### Распределение Хи-квадрат (Chi-squared) Пирсона для связи между двумя номинативными переменными

> Примечание по использованию критерия! При анализе четырехпольных таблиц ожидаемые значения в каждой из ячеек должны быть не менее 10. В том случае, если хотя бы в одной ячейке ожидаемое явление принимает значение от 5 до 9, критерий хи-квадрат должен рассчитываться с поправкой Йейтса. Если хотя бы в одной ячейке ожидаемое явление меньше 5, то для анализа должен использоваться точный критерий Фишера. В случае анализа многопольных таблиц ожидаемое число наблюдений не должно принимать значения менее 5 более чем в 20% ячеек.

<pre>
<em># Создаем таблицу сопряженности (contingency table) для двух номинативных переменных</em>
contingency_table = np.array([
    [10, 6],
    [5, 15]
])
<em># Вычисляем статистику Хи-квадрат, p-значение и степени свободы</em>
chi2_stat, p_value, dof, expected_freq = chi2_contingency(contingency_table)
<em># Задаем пороговый уровень значимости</em>
alpha = 0.05
print("Статистика Хи-квадрат:", chi2_stat)
print("P-значение:", p_value)
print("Степени свободы:", dof)
print("\nГипотезы:")
print("H0: нет связи между двумя номинативными переменными")
print("H1: есть связь между двумя номинативными переменными")
print("\nРезультат:")
if p_value < alpha:
    print(f"Отвергаем H0 на уровне значимости {alpha}. Есть связь между двумя номинативными переменными.")
else:
    print(f"Не можем отвергнуть H0 на уровне значимости {alpha}. Нет связи между двумя номинативными переменными.")
</pre>
Вывод:

<pre>
Статистика Хи-квадрат: 3.715714285714286
P-значение: 0.053902557169387154
Степени свободы: 1

Гипотезы:
H0: нет связи между двумя номинативными переменными
H1: есть связь между двумя номинативными переменными

Результат:
Не можем отвергнуть H0 на уровне значимости 0.05. Нет связи между двумя номинативными переменными.
</pre>
> Критерий использует таблицу сопряженности для определения, есть ли статистически значимая связь между двумя категориальными переменными. В этом случае, вы сравниваете наблюдаемые частоты с частотами, ожидаемыми при условии независимости переменных. Формула расчета та же, что и для одной номинативной переменной
